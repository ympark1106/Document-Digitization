{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from gensim.models.callbacks import PerplexityMetric, CallbackAny2Vec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gpu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gpu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentences containing the word 'pavement' to create new corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  \n",
    "            sentence = ' '.join(tokens) \n",
    "            if target_word in sentence:\n",
    "                sentences_with_word.append(sentence)\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(input_folder, output_folder, target_word):\n",
    "    files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.join(output_folder, os.path.basename(file_path)[:-4] + f'_containing_{target_word}.txt')\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_corpus_folder = \"Corpus_Thesaurus_textfiles/corpus_number_removed\"\n",
    "    output_corpus_folder = \"Val_Textfiles/val_corpus/corpus_containing_pavement\"\n",
    "    target_word = \"pavement\"  \n",
    "\n",
    "    create_new_files(input_corpus_folder, output_corpus_folder, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 300/300 [04:12<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "synonyms = {}\n",
    "\n",
    "corpus_folder_path = 'Val_Textfiles/val_corpus/corpus_containing_pavement'\n",
    "\n",
    "corpus_sentences = []\n",
    "\n",
    "for file_name in os.listdir(corpus_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text) \n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                words = [word for word in words if word not in stop_words] \n",
    "                corpus_sentences.append(words)\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_sentences, vector_size=200, window=5, \n",
    "                               min_count=3, workers=4, sg=1, epochs=300)\n",
    "\n",
    "epochs = 300\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "    model.train(corpus_sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    synonyms[word] = [(similar_word[0], similar_word[1]) for similar_word in similar_words]\n",
    "\n",
    "output_file = 'd_r_thesaurus_containing_pavement_500.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in synonyms:\n",
    "        similar_words = synonyms[word]\n",
    "        line = f'{word}: ' + ', '.join([f'{similar_word[0]}({similar_word[1]:.2f})' for similar_word in similar_words])\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_thesaurus_file = 'Val_Textfiles/val_thesaurus/d_r_thesaurus_containing_pavement_300.txt'\n",
    "\n",
    "new_embedding_thesaurus_file = 'd_r_thesaurus_containing_pavement_300_0.5.txt'\n",
    "selected_entries = [] \n",
    "\n",
    "with open(embedding_thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        selected_synonyms = []\n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            if similarity >= 0.5:\n",
    "                selected_synonyms.append(f\"{synonym_word}({similarity})\")\n",
    "        if selected_synonyms:\n",
    "            selected_entries.append(f\"{word}: {', '.join(selected_synonyms)}\")\n",
    "\n",
    "with open(new_embedding_thesaurus_file, 'w', encoding='utf-8') as file:\n",
    "    for entry in selected_entries:\n",
    "        file.write(entry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "d_r_thesaurus_300_0.5_network.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"d_r_thesaurus_300_0.5_network.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fea5a8837d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thesaurus_file = 'd_r_thesaurus_containing_pavement_300_0.5.txt'\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        G.add_node(word)  \n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            G.add_edge(word, synonym_word, weight=similarity)  \n",
    "\n",
    "layout = nx.spring_layout(G, seed=42)\n",
    "\n",
    "nt = Network(notebook=True, height='800px', width='100%')\n",
    "\n",
    "for node in G.nodes():\n",
    "    nt.add_node(node, label=node, title=node)\n",
    "\n",
    "for edge in G.edges():\n",
    "    source, target, weight = edge[0], edge[1], G.edges[edge]['weight']\n",
    "    nt.add_edge(source, target, value=weight, title=f'Weight: {weight}')\n",
    "\n",
    "output_file = \"d_r_thesaurus_300_0.5_network.html\"\n",
    "\n",
    "nt.show(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
