{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from gensim.models.callbacks import PerplexityMetric, CallbackAny2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gpu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gpu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/gpu/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/gpu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentences containing the word 'pavement' to create new corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  \n",
    "            sentence = ' '.join(tokens) \n",
    "            if target_word in sentence:\n",
    "                sentences_with_word.append(sentence)\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(input_folder, output_folder, target_word):\n",
    "    files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.join(output_folder, os.path.basename(file_path)[:-4] + f'_containing_{target_word}.txt')\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_corpus_folder = \"Corpus_Thesaurus_textfiles/corpus_number_removed\"\n",
    "    output_corpus_folder = \"Val_Textfiles/val_corpus/corpus_containing_pavement\"\n",
    "    target_word = \"pavement\"  \n",
    "\n",
    "    create_new_files(input_corpus_folder, output_corpus_folder, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 500/500 [07:09<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "synonyms = {}\n",
    "\n",
    "corpus_folder_path = 'Val_Textfiles/val_corpus/corpus_containing_pavement'\n",
    "\n",
    "corpus_sentences = []\n",
    "\n",
    "for file_name in os.listdir(corpus_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text) \n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                words = [word for word in words if word not in stop_words] \n",
    "                corpus_sentences.append(words)\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_sentences, vector_size=200, window=5, \n",
    "                               min_count=3, workers=4, sg=1, epochs=500)\n",
    "\n",
    "epochs = 500\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "    model.train(corpus_sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    synonyms[word] = [(similar_word[0], similar_word[1]) for similar_word in similar_words]\n",
    "\n",
    "output_file = 'd_r_thesaurus_containing_pavement_500.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in synonyms:\n",
    "        similar_words = synonyms[word]\n",
    "        line = f'{word}: ' + ', '.join([f'{similar_word[0]}({similar_word[1]:.2f})' for similar_word in similar_words])\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_thesaurus_file = 'Val_Textfiles/val_thesaurus/d_r_thesaurus_containing_pavement_500.txt'\n",
    "\n",
    "new_embedding_thesaurus_file = 'd_r_thesaurus_containing_pavement_500_0.5.txt'\n",
    "selected_entries = [] \n",
    "\n",
    "with open(embedding_thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        selected_synonyms = []\n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            if similarity >= 0.5:\n",
    "                selected_synonyms.append(f\"{synonym_word}({similarity})\")\n",
    "        if selected_synonyms:\n",
    "            selected_entries.append(f\"{word}: {', '.join(selected_synonyms)}\")\n",
    "\n",
    "with open(new_embedding_thesaurus_file, 'w', encoding='utf-8') as file:\n",
    "    for entry in selected_entries:\n",
    "        file.write(entry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus_file = 'd_r_thesaurus_containing_pavement_500_0.5.txt'\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        G.add_node(word)  \n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            G.add_edge(word, synonym_word, weight=similarity)  \n",
    "\n",
    "layout = nx.spring_layout(G, seed=42)\n",
    "\n",
    "nt = Network(notebook=True, height='800px', width='100%')\n",
    "\n",
    "for node in G.nodes():\n",
    "    nt.add_node(node, label=node, title=node)\n",
    "\n",
    "for edge in G.edges():\n",
    "    source, target, weight = edge[0], edge[1], G.edges[edge]['weight']\n",
    "    nt.add_edge(source, target, value=weight, title=f'Weight: {weight}')\n",
    "\n",
    "output_file = \"d_r_thesaurus_500_0.5_network.html\"\n",
    "\n",
    "nt.show(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract \"definition of pavement\" in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = 'Val_Textfiles/val_corpus/corpus_containing_pavement'\n",
    "output_folder = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "def extract_definition_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = content.split('.')\n",
    "        \n",
    "        definition_sentences = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if 'definition' in sentence.lower():\n",
    "                start_idx = max(0, i - 2)\n",
    "                end_idx = min(len(sentences), i + 3)\n",
    "                extracted_sentences = sentences[start_idx:end_idx]\n",
    "                definition_sentences.append(' '.join(extracted_sentences).strip())\n",
    "                \n",
    "        return definition_sentences\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        definition_sentences = extract_definition_sentences(file_path)\n",
    "        output_file_path = os.path.join(output_folder, f'{file_name}_definition.txt')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            for sentence in definition_sentences:\n",
    "                output_file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 0_Canada_Ontario_2018_containing_pavement_definition.txt and 0_Australia_Northern Territory_2017_containing_pavement_definition.txt: 0.6365\n",
      "Similarity between 0_Canada_Ontario_2018_containing_pavement_definition.txt and 0_Canada_British Columbia_2016_containing_pavement.txt_definition.txt: 0.5620\n",
      "Similarity between 0_Australia_Northern Territory_2017_containing_pavement_definition.txt and 0_Canada_Ontario_2018_containing_pavement_definition.txt: 0.6365\n",
      "Similarity between 0_Australia_Northern Territory_2017_containing_pavement_definition.txt and 0_Canada_British Columbia_2016_containing_pavement.txt_definition.txt: 0.5802\n",
      "Similarity between 0_Canada_British Columbia_2016_containing_pavement.txt_definition.txt and 0_Canada_Ontario_2018_containing_pavement_definition.txt: 0.5620\n",
      "Similarity between 0_Canada_British Columbia_2016_containing_pavement.txt_definition.txt and 0_Australia_Northern Territory_2017_containing_pavement_definition.txt: 0.5802\n"
     ]
    }
   ],
   "source": [
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()  \n",
    "        sentences = text.split('.')  \n",
    "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "folder_path = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "sentences_per_file = {file_path: get_sentences_from_file(file_path) for file_path in file_paths}\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "for file1, sentences1 in sentences_per_file.items():\n",
    "    for file2, sentences2 in sentences_per_file.items():\n",
    "        if file1 != file2: \n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "            file1_name = os.path.basename(file1)\n",
    "            file2_name = os.path.basename(file2)\n",
    "\n",
    "            similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "            average_similarity = similarity_matrix.mean().item()\n",
    "\n",
    "            print(f\"Similarity between {file1_name} and {file2_name}: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus_path = 'Val_Textfiles/val_thesaurus/d_r_thesaurus_containing_pavement_500.txt'\n",
    "with open(thesaurus_path, 'r', encoding='utf-8') as thesaurus_file:\n",
    "    thesaurus_data = {}\n",
    "    for line in thesaurus_file:\n",
    "        parts = line.split(':')\n",
    "        base_word = parts[0].strip()\n",
    "        synonyms_with_similarity = [syn.strip().split('(') for syn in parts[1].split(',')]\n",
    "        synonyms = [(syn[0], float(syn[1].replace(')', ''))) for syn in synonyms_with_similarity]\n",
    "        thesaurus_data[base_word] = synonyms\n",
    "\n",
    "def replace_words_with_thesaurus(word, thesaurus, similarity_threshold=0.3):\n",
    "    best_synonym = word\n",
    "    best_similarity = similarity_threshold \n",
    "\n",
    "    for synonym, similarity in thesaurus:\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_synonym = synonym\n",
    "\n",
    "    return best_synonym\n",
    "\n",
    "def process_file(file_path, thesaurus):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        sentences = text.split('.')\n",
    "        replaced_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            replaced_words = [replace_words_with_thesaurus(word, thesaurus.get(word, [(word, 1.0)])) for word in sentence.split()]\n",
    "            replaced_sentence = ' '.join(replaced_words)\n",
    "            replaced_sentences.append(replaced_sentence)\n",
    "\n",
    "    new_file_path = os.path.join(new_folder_path, '1' + os.path.basename(file_path))\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "        new_file.write('. '.join(replaced_sentences))\n",
    "\n",
    "new_folder_path = 'Val_Textfiles/val_corpus/Switched_definition_of_pavement_in_corpus'\n",
    "folder_path = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        process_file(file_path, thesaurus_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 10_Canada_British Columbia_2016_containing_pavement.txt_definition.txt and 10_Australia_Northern Territory_2017_containing_pavement_definition.txt: 0.2508\n",
      "Similarity between 10_Canada_British Columbia_2016_containing_pavement.txt_definition.txt and 10_Canada_Ontario_2018_containing_pavement_definition.txt: 0.2058\n",
      "Similarity between 10_Australia_Northern Territory_2017_containing_pavement_definition.txt and 10_Canada_British Columbia_2016_containing_pavement.txt_definition.txt: 0.2508\n",
      "Similarity between 10_Australia_Northern Territory_2017_containing_pavement_definition.txt and 10_Canada_Ontario_2018_containing_pavement_definition.txt: 0.5003\n",
      "Similarity between 10_Canada_Ontario_2018_containing_pavement_definition.txt and 10_Canada_British Columbia_2016_containing_pavement.txt_definition.txt: 0.2058\n",
      "Similarity between 10_Canada_Ontario_2018_containing_pavement_definition.txt and 10_Australia_Northern Territory_2017_containing_pavement_definition.txt: 0.5003\n"
     ]
    }
   ],
   "source": [
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()  \n",
    "        sentences = text.split('.')  \n",
    "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "folder_path = 'Val_Textfiles/val_corpus/Switched_definition_of_pavement_in_corpus'\n",
    "\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "sentences_per_file = {file_path: get_sentences_from_file(file_path) for file_path in file_paths}\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "for file1, sentences1 in sentences_per_file.items():\n",
    "    for file2, sentences2 in sentences_per_file.items():\n",
    "        if file1 != file2: \n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "            file1_name = os.path.basename(file1)\n",
    "            file2_name = os.path.basename(file2)\n",
    "\n",
    "            similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "            average_similarity = similarity_matrix.mean().item()\n",
    "\n",
    "            print(f\"Similarity between {file1_name} and {file2_name}: {average_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
