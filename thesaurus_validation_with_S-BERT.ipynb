{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(0)\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from gensim.models.callbacks import PerplexityMetric, CallbackAny2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  \n",
    "            sentence = ' '.join(tokens) \n",
    "            if target_word in sentence:\n",
    "                sentences_with_word.append(sentence)\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(input_folder, output_folder, target_word):\n",
    "    files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.join(output_folder, os.path.basename(file_path)[:-4] + f'_containing_{target_word}.txt')\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_corpus_folder = \"Corpus_Thesaurus_textfiles/corpus_number_removed\"\n",
    "    output_corpus_folder = \"Val_Textfiles/val_corpus/corpus_containing_definition\"\n",
    "    target_word = \"definition\"  \n",
    "\n",
    "    create_new_files(input_corpus_folder, output_corpus_folder, target_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use first files in each corpus(OG) for calculating sentence similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarities: 100%|██████████| 80283/80283 [20:36<00:00, 64.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "def calculate_and_save_similarity(file_path_a, file_path_b, output_file_path):\n",
    "    sentences_a = get_sentences_from_file(file_path_a)\n",
    "    sentences_b = get_sentences_from_file(file_path_b)\n",
    "\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2').cuda()\n",
    "\n",
    "    # exit()\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        total_similarities = len(sentences_a) * len(sentences_b)\n",
    "        pbar = tqdm(total=total_similarities, desc=\"Calculating Similarities\")\n",
    "\n",
    "        for sentence_a in sentences_a:\n",
    "            for sentence_b in sentences_b:\n",
    "                embeddings_a = model.encode(sentence_a, convert_to_tensor=True)\n",
    "                embeddings_b = model.encode(sentence_b, convert_to_tensor=True)\n",
    "\n",
    "                similarity_score = util.pytorch_cos_sim(embeddings_a, embeddings_b).item()\n",
    "\n",
    "                output_file.write(f\"Similarity between '{sentence_a}' and '{sentence_b}': {similarity_score:.4f}\\n\")\n",
    "                pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "file_path_a = 'Val_Textfiles/val_corpus/corpus_definition_onlyfirstfile/Australia_Northern Territory_2017_containing_definition.txt'\n",
    "file_path_b = 'Val_Textfiles/val_corpus/corpus_og_onlyfirstfile/Australia_Northern Territory_2017.txt'\n",
    "output_file_path = '0_evaluation_with_S-BERT_sentences.txt'\n",
    "\n",
    "calculate_and_save_similarity(file_path_a, file_path_b, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Replace each word with a similar word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "thesaurus_path = 'Corpus_Thesaurus_textfiles/d_r_based_thesaurus_files/d_r_based_thesaurus_20015_epoch500.txt'\n",
    "with open(thesaurus_path, 'r', encoding='utf-8') as thesaurus_file:\n",
    "    thesaurus_data = {}\n",
    "    for line in thesaurus_file:\n",
    "        parts = line.split(':')\n",
    "        base_word = parts[0].strip()\n",
    "        synonyms_with_similarity = [syn.strip().split('(') for syn in parts[1].split(',')]\n",
    "        synonyms = [(syn[0], float(syn[1].replace(')', ''))) for syn in synonyms_with_similarity]\n",
    "        thesaurus_data[base_word] = synonyms\n",
    "\n",
    "def replace_words_with_thesaurus(word, thesaurus, similarity_threshold=0):\n",
    "    best_synonym = word\n",
    "    best_similarity = similarity_threshold \n",
    "\n",
    "    for synonym, similarity in thesaurus:\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_synonym = synonym\n",
    "\n",
    "    return best_synonym\n",
    "\n",
    "def process_file(file_path, thesaurus):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        sentences = text.split('\\n')  \n",
    "        replaced_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            replaced_words = [replace_words_with_thesaurus(word, thesaurus.get(word, [(word, 1.0)])) for word in sentence.split()]\n",
    "            replaced_sentence = ' '.join(replaced_words).strip()\n",
    "            if replaced_sentence:\n",
    "                replaced_sentences.append(replaced_sentence)\n",
    "\n",
    "        new_file_path = os.path.join(new_folder_path, 'Replaced_' + os.path.basename(file_path)) \n",
    "        with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "            new_file.write('\\n'.join(replaced_sentences))\n",
    "\n",
    "new_folder_path = 'Val_Textfiles/val_corpus/switched_corpus_only_definition'\n",
    "folder_path = 'Val_Textfiles/val_corpus/corpus_definition_onlyfirstfile'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        process_file(file_path, thesaurus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "thesaurus_path = 'Corpus_Thesaurus_textfiles/d_r_based_thesaurus_files/d_r_based_thesaurus_20015_epoch500.txt'\n",
    "with open(thesaurus_path, 'r', encoding='utf-8') as thesaurus_file:\n",
    "    thesaurus_data = {}\n",
    "    for line in thesaurus_file:\n",
    "        parts = line.split(':')\n",
    "        base_word = parts[0].strip()\n",
    "        synonyms_with_similarity = [syn.strip().split('(') for syn in parts[1].split(',')]\n",
    "        synonyms = [(syn[0], float(syn[1].replace(')', ''))) for syn in synonyms_with_similarity]\n",
    "        thesaurus_data[base_word] = synonyms\n",
    "\n",
    "def replace_words_with_thesaurus(word, thesaurus, similarity_threshold=0):\n",
    "    best_synonym = word\n",
    "    best_similarity = similarity_threshold \n",
    "\n",
    "    for synonym, similarity in thesaurus:\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_synonym = synonym\n",
    "\n",
    "    return best_synonym\n",
    "\n",
    "def process_file(file_path, thesaurus):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        sentences = text.split('\\n')  \n",
    "        replaced_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            replaced_words = [replace_words_with_thesaurus(word, thesaurus.get(word, [(word, 1.0)])) for word in sentence.split()]\n",
    "            replaced_sentence = ' '.join(replaced_words).strip()\n",
    "            if replaced_sentence:\n",
    "                replaced_sentences.append(replaced_sentence)\n",
    "\n",
    "        new_file_path = os.path.join(new_folder_path, 'Replaced_' + os.path.basename(file_path)) \n",
    "        with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "            new_file.write('\\n'.join(replaced_sentences))\n",
    "\n",
    "new_folder_path = 'Val_Textfiles/val_corpus/switched_corpus_og'\n",
    "folder_path = 'Val_Textfiles/val_corpus/corpus_og_onlyfirstfile'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        process_file(file_path, thesaurus_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use first files in each corpus(Replaced) for calculating sentence similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Similarities: 100%|██████████| 80283/80283 [21:26<00:00, 62.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        return [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "def calculate_and_save_similarity(file_path_a, file_path_b, output_file_path):\n",
    "    sentences_a = get_sentences_from_file(file_path_a)\n",
    "    sentences_b = get_sentences_from_file(file_path_b)\n",
    "\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        total_similarities = len(sentences_a) * len(sentences_b)\n",
    "        pbar = tqdm(total=total_similarities, desc=\"Calculating Similarities\")\n",
    "\n",
    "        for sentence_a in sentences_a:\n",
    "            for sentence_b in sentences_b:\n",
    "                embeddings_a = model.encode(sentence_a, convert_to_tensor=True)\n",
    "                embeddings_b = model.encode(sentence_b, convert_to_tensor=True)\n",
    "\n",
    "                similarity_score = util.pytorch_cos_sim(embeddings_a, embeddings_b).item()\n",
    "\n",
    "                output_file.write(f\"Similarity between '{sentence_a}' and '{sentence_b}': {similarity_score:.4f}\\n\")\n",
    "                pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "file_path_a = 'Val_Textfiles/val_corpus/switched_corpus_only_definition/Replaced_Australia_Northern Territory_2017_containing_definition.txt'\n",
    "file_path_b = 'Val_Textfiles/val_corpus/switched_corpus_og/Replaced_Australia_Northern Territory_2017.txt'\n",
    "output_file_path = '1_evaluation_with_S-BERT_sentences.txt'\n",
    "\n",
    "calculate_and_save_similarity(file_path_a, file_path_b, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of similarity between the original corpus and the word-substituted corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_a = '0_evaluation_with_S-BERT_sentences.txt'\n",
    "file_path_b = '1_evaluation_with_S-BERT_sentences.txt'\n",
    "output_merged_file_path = 'output_merged_lines.txt'\n",
    "\n",
    "with open(file_path_a, 'r', encoding='utf-8') as file_a, \\\n",
    "     open(file_path_b, 'r', encoding='utf-8') as file_b, \\\n",
    "     open(output_merged_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    \n",
    "    lines_a = file_a.readlines()\n",
    "    lines_b = file_b.readlines()\n",
    "\n",
    "    for line_a, line_b in zip(lines_a, lines_b):\n",
    "        output_file.write(f\"{line_a.strip()}\\n{line_b.strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lines_by_similarity(lines):\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line_a = lines[i].strip()\n",
    "        line_b = lines[i + 1].strip()\n",
    "\n",
    "        similarity_score = float(line_b[-6:])\n",
    "\n",
    "        if similarity_score < 0.5:\n",
    "            del lines[i:i + 2]\n",
    "        else:\n",
    "            i += 2  \n",
    "\n",
    "    return lines\n",
    "\n",
    "output_merged_file_path = 'output_merged_lines.txt'\n",
    "\n",
    "with open(output_merged_file_path, 'r', encoding='utf-8') as output_file:\n",
    "    merged_lines = output_file.readlines()\n",
    "\n",
    "    filtered_lines = filter_lines_by_similarity(merged_lines)\n",
    "\n",
    "with open(output_merged_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.writelines(filtered_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_remove_lines(lines):\n",
    "    i = 0\n",
    "    while i < len(lines) - 1:  # Ensure there is a pair of lines to compare\n",
    "        line_a = lines[i].strip()\n",
    "        line_b = lines[i + 1].strip()\n",
    "\n",
    "        similarity_a = float(line_a[-6:])\n",
    "        similarity_b = float(line_b[-6:])\n",
    "\n",
    "        if similarity_b < similarity_a:\n",
    "            # Remove the pair of lines\n",
    "            del lines[i:i + 2]\n",
    "        else:\n",
    "            i += 2  # Move to the next pair of lines\n",
    "\n",
    "    return lines\n",
    "\n",
    "# Example usage:\n",
    "output_merged_file_path = 'output_merged_lines_th0.5.txt'\n",
    "\n",
    "with open(output_merged_file_path, 'r', encoding='utf-8') as output_file:\n",
    "    merged_lines = output_file.readlines()\n",
    "\n",
    "    # Filter and remove lines based on similarity\n",
    "    filtered_lines = filter_lines_by_similarity(merged_lines)\n",
    "\n",
    "    # Compare and remove lines based on the new criteria\n",
    "    final_filtered_lines = compare_and_remove_lines(filtered_lines)\n",
    "\n",
    "# Write the final filtered lines back to the merged file\n",
    "with open(output_merged_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.writelines(final_filtered_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentences containing the word 'pavement' to create new corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  \n",
    "            sentence = ' '.join(tokens) \n",
    "            if target_word in sentence:\n",
    "                sentences_with_word.append(sentence)\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(input_folder, output_folder, target_word):\n",
    "    files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.join(output_folder, os.path.basename(file_path)[:-4] + f'_containing_{target_word}.txt')\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_corpus_folder = \"Corpus_Thesaurus_textfiles/corpus_number_removed\"\n",
    "    output_corpus_folder = \"Val_Textfiles/val_corpus/corpus_containing_pavement\"\n",
    "    target_word = \"pavement\"  \n",
    "\n",
    "    create_new_files(input_corpus_folder, output_corpus_folder, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "synonyms = {}\n",
    "\n",
    "corpus_folder_path = 'Val_Textfiles/val_corpus/corpus_containing_pavement'\n",
    "\n",
    "corpus_sentences = []\n",
    "\n",
    "for file_name in os.listdir(corpus_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text) \n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                words = [word for word in words if word not in stop_words] \n",
    "                corpus_sentences.append(words)\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_sentences, vector_size=200, window=10, \n",
    "                               min_count=5, workers=5, sg=1, epochs=300) \n",
    "                                #corpus_sentences, vector_size=200, window=5, min_count=3, workers=4, sg=1, epochs=500\n",
    "                                #corpus_sentences, vector_size=300, window=5, min_count=5, workers=4, sg=1, epochs=500\n",
    "epochs = 500\n",
    "for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "    model.train(corpus_sentences, total_examples=model.corpus_count, epochs=1)\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    synonyms[word] = [(similar_word[0], similar_word[1]) for similar_word in similar_words]\n",
    "\n",
    "output_file = 'd_r_thesaurus_containing_pavement_300_5_500.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in synonyms:\n",
    "        similar_words = synonyms[word]\n",
    "        line = f'{word}: ' + ', '.join([f'{similar_word[0]}({similar_word[1]:.2f})' for similar_word in similar_words])\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_thesaurus_file = 'Val_Textfiles/val_thesaurus/d_r_thesaurus_containing_pavement_500.txt'\n",
    "\n",
    "new_embedding_thesaurus_file = 'd_r_thesaurus_containing_pavement_500_0.5.txt'\n",
    "selected_entries = [] \n",
    "\n",
    "with open(embedding_thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        selected_synonyms = []\n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            if similarity >= 0.5:\n",
    "                selected_synonyms.append(f\"{synonym_word}({similarity})\")\n",
    "        if selected_synonyms:\n",
    "            selected_entries.append(f\"{word}: {', '.join(selected_synonyms)}\")\n",
    "\n",
    "with open(new_embedding_thesaurus_file, 'w', encoding='utf-8') as file:\n",
    "    for entry in selected_entries:\n",
    "        file.write(entry + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus_file = 'd_r_thesaurus_containing_pavement_500_0.5.txt'\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "with open(thesaurus_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        word, synonyms_str = line.split(': ', 1)\n",
    "        synonyms_list = synonyms_str.split(', ')\n",
    "        G.add_node(word)  \n",
    "        for synonym in synonyms_list:\n",
    "            synonym_word, similarity = synonym.split('(')\n",
    "            similarity = float(similarity[:-1])\n",
    "            G.add_edge(word, synonym_word, weight=similarity)  \n",
    "\n",
    "layout = nx.spring_layout(G, seed=42)\n",
    "\n",
    "nt = Network(notebook=True, height='800px', width='100%')\n",
    "\n",
    "for node in G.nodes():\n",
    "    nt.add_node(node, label=node, title=node)\n",
    "\n",
    "for edge in G.edges():\n",
    "    source, target, weight = edge[0], edge[1], G.edges[edge]['weight']\n",
    "    nt.add_edge(source, target, value=weight, title=f'Weight: {weight}')\n",
    "\n",
    "output_file = \"d_r_thesaurus_500_0.5_network.html\"\n",
    "\n",
    "nt.show(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract \"definition of pavement\" in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = 'Val_Textfiles/val_corpus/corpus_containing_pavement'\n",
    "output_folder = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "def extract_definition_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = content.split('.')\n",
    "        \n",
    "        definition_sentences = []\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if 'definition' in sentence.lower():\n",
    "                start_idx = max(0, i - 2)\n",
    "                end_idx = min(len(sentences), i + 3)\n",
    "                extracted_sentences = sentences[start_idx:end_idx]\n",
    "                definition_sentences.append(' '.join(extracted_sentences).strip())\n",
    "                \n",
    "        return definition_sentences\n",
    "\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        definition_sentences = extract_definition_sentences(file_path)\n",
    "        output_file_path = os.path.join(output_folder, f'{file_name}_definition.txt')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            for sentence in definition_sentences:\n",
    "                output_file.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()  \n",
    "        sentences = text.split('.')  \n",
    "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "folder_path = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "sentences_per_file = {file_path: get_sentences_from_file(file_path) for file_path in file_paths}\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "for file1, sentences1 in sentences_per_file.items():\n",
    "    for file2, sentences2 in sentences_per_file.items():\n",
    "        if file1 != file2: \n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "            file1_name = os.path.basename(file1)\n",
    "            file2_name = os.path.basename(file2)\n",
    "\n",
    "            similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "            average_similarity = similarity_matrix.mean().item()\n",
    "\n",
    "            print(f\"Similarity between {file1_name} and {file2_name}: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus_path = 'Val_Textfiles/val_thesaurus/d_r_thesaurus_containing_pavement_300_10.txt'\n",
    "with open(thesaurus_path, 'r', encoding='utf-8') as thesaurus_file:\n",
    "    thesaurus_data = {}\n",
    "    for line in thesaurus_file:\n",
    "        parts = line.split(':')\n",
    "        base_word = parts[0].strip()\n",
    "        synonyms_with_similarity = [syn.strip().split('(') for syn in parts[1].split(',')]\n",
    "        synonyms = [(syn[0], float(syn[1].replace(')', ''))) for syn in synonyms_with_similarity]\n",
    "        thesaurus_data[base_word] = synonyms\n",
    "\n",
    "def replace_words_with_thesaurus(word, thesaurus, similarity_threshold=0.7): #수정\n",
    "    best_synonym = word\n",
    "    best_similarity = similarity_threshold \n",
    "\n",
    "    for synonym, similarity in thesaurus:\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_synonym = synonym\n",
    "\n",
    "    return best_synonym\n",
    "\n",
    "def process_file(file_path, thesaurus):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        sentences = text.split('.')\n",
    "        replaced_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            replaced_words = [replace_words_with_thesaurus(word, thesaurus.get(word, [(word, 1.0)])) for word in sentence.split()]\n",
    "            replaced_sentence = ' '.join(replaced_words)\n",
    "            replaced_sentences.append(replaced_sentence)\n",
    "\n",
    "    new_file_path = os.path.join(new_folder_path, '0.7_' + os.path.basename(file_path)) #수정\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "        new_file.write('. '.join(replaced_sentences))\n",
    "\n",
    "new_folder_path = 'Val_Textfiles/val_corpus/Switched_definition_of_pavement_in_corpus'\n",
    "folder_path = 'Val_Textfiles/val_corpus/definition_of_pavement_in_corpus'\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        process_file(file_path, thesaurus_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()  \n",
    "        sentences = text.split('.')  \n",
    "        return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "folder_path = 'Val_Textfiles/val_corpus/Switched_definition_of_pavement_in_corpus'\n",
    "\n",
    "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith('.txt')]\n",
    "\n",
    "sentences_per_file = {file_path: get_sentences_from_file(file_path) for file_path in file_paths}\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "for file1, sentences1 in sentences_per_file.items():\n",
    "    for file2, sentences2 in sentences_per_file.items():\n",
    "        if file1 != file2: \n",
    "            embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "            file1_name = os.path.basename(file1)\n",
    "            file2_name = os.path.basename(file2)\n",
    "\n",
    "            similarity_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "            average_similarity = similarity_matrix.mean().item()\n",
    "\n",
    "            print(f\"Similarity between {file1_name} and {file2_name}: {average_similarity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
