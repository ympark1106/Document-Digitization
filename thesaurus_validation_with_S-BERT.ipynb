{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from gensim.models.callbacks import PerplexityMetric, CallbackAny2Vec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            sentences = line.split('.') \n",
    "            for sentence in sentences:\n",
    "                if target_word in sentence:\n",
    "                    sentences_with_word.append(sentence.strip())\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(corpus_folder, target_word):\n",
    "    files = glob.glob(os.path.join(corpus_folder, '*.txt'))\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.splitext(file_path)[0] + f'_containing_{target_word}.txt'\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus_folder = \"Corpus_Thesaurus_textfiles\\corpus_containing_pavement\"\n",
    "    target_word = \"pavement\" \n",
    "\n",
    "    create_new_files(corpus_folder, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_with_word(file_path, target_word):\n",
    "    sentences_with_word = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            tokens = line.strip().split()  \n",
    "            sentence = ' '.join(tokens) \n",
    "            if target_word in sentence:\n",
    "                sentences_with_word.append(sentence)\n",
    "    return sentences_with_word\n",
    "\n",
    "def create_new_files(input_folder, output_folder, target_word):\n",
    "    files = glob.glob(os.path.join(input_folder, '*.txt'))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_path in files:\n",
    "        sentences_with_word = extract_sentences_with_word(file_path, target_word)\n",
    "        \n",
    "        if sentences_with_word:\n",
    "            new_file_path = os.path.join(output_folder, os.path.basename(file_path)[:-4] + f'_containing_{target_word}.txt')\n",
    "            \n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                for sentence in sentences_with_word:\n",
    "                    new_file.write(sentence + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_corpus_folder = \"Corpus_Thesaurus_textfiles\\corpus_number_removed\"\n",
    "    output_corpus_folder = \"Corpus_Thesaurus_textfiles\\corpus_containing_pavement\"\n",
    "    target_word = \"pavement\"  \n",
    "\n",
    "    create_new_files(input_corpus_folder, output_corpus_folder, target_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes exactly one argument (the instance to initialize)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive - gnu.ac.kr\\박유민\\PlayGround\\Document-Digitization\\thesaurus_validation_with_S-BERT.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m                 corpus_sentences\u001b[39m.\u001b[39mappend(words)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Initialize Word2Vec Model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mWord2Vec(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     corpus_sentences, vector_size\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     min_count\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sg\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[PerplexityCallback(corpus\u001b[39m=\u001b[39;49mcorpus_sentences, progress_per\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)]  \u001b[39m# Adjust 'progress_per' as needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Retrieve Synonyms\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mwv\u001b[39m.\u001b[39mindex_to_key:\n",
      "\u001b[1;32mc:\\Users\\user\\OneDrive - gnu.ac.kr\\박유민\\PlayGround\\Document-Digitization\\thesaurus_validation_with_S-BERT.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# 부모 클래스의 초기화 메서드 호출\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# 'corpus' 키워드 인자를 뽑아내어 self.corpus에 저장\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mcorpus\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: object.__init__() takes exactly one argument (the instance to initialize)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.callbacks import PerplexityMetric, CallbackAny2Vec\n",
    "\n",
    "class PerplexityCallback(CallbackAny2Vec):\n",
    "    def __init__(self, corpus, *args, **kwargs):\n",
    "        super().__init__(self, *args, **kwargs)\n",
    "        self.corpus = corpus\n",
    "        self.perplexity_values = []\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        perplexity = PerplexityMetric(corpus=self.corpus)\n",
    "        perplexity_value = perplexity.get_perplexity()\n",
    "        self.perplexity_values.append(perplexity_value)\n",
    "        print(f\"Epoch #{self.epoch} - Perplexity: {perplexity_value:.2f}\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "synonyms = {}\n",
    "\n",
    "corpus_folder_path = 'Corpus_Thesaurus_textfiles\\corpus_containing_pavement'\n",
    "corpus_sentences = []\n",
    "\n",
    "for file_name in os.listdir(corpus_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text) \n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                words = [word for word in words if word not in stop_words] \n",
    "                corpus_sentences.append(words)\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    corpus_sentences, vector_size=200, window=5,\n",
    "    min_count=3, workers=4, sg=1, epochs=100,\n",
    "    callbacks=[PerplexityCallback(corpus=corpus_sentences, progress_per=10)]\n",
    ")\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    synonyms[word] = [(similar_word[0], similar_word[1]) for similar_word in similar_words]\n",
    "\n",
    "output_file = 'd_r_thesaurus_containing_pavement.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in synonyms:\n",
    "        similar_words = synonyms[word]\n",
    "        line = f'{word}: ' + ', '.join([f'{similar_word[0]}({similar_word[1]:.2f})' for similar_word in similar_words])\n",
    "        file.write(line + '\\n')\n",
    "\n",
    "print(\"Distributed Representation-based Thesaurus Construction is completed.\")\n",
    "\n",
    "epochs = range(1, len(model.callbacks[0].perplexity_values) + 1)\n",
    "perplexity_values = model.callbacks[0].perplexity_values\n",
    "\n",
    "plt.plot(epochs, perplexity_values, marker='o')\n",
    "plt.title('Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive - gnu.ac.kr\\박유민\\PlayGround\\Document-Digitization\\thesaurus_validation_with_S-BERT.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sentences \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39msent_tokenize(text) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(sentence\u001b[39m.\u001b[39;49mlower())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive%20-%20gnu.ac.kr/%EB%B0%95%EC%9C%A0%EB%AF%BC/PlayGround/Document-Digitization/thesaurus_validation_with_S-BERT.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     corpus_sentences\u001b[39m.\u001b[39mappend(words)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m sentences \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39;49mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nltk\\tokenize\\destructive.py:157\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    149\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mreturn_str\u001b[39m\u001b[39m'\u001b[39m\u001b[39m has been deprecated and should no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlonger be used.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    152\u001b[0m         category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[0;32m    153\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTARTING_QUOTES:\n\u001b[1;32m--> 157\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39;49msub(substitution, text)\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m regexp, substitution \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mPUNCTUATION:\n\u001b[0;32m    160\u001b[0m     text \u001b[39m=\u001b[39m regexp\u001b[39m.\u001b[39msub(substitution, text)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "synonyms = {}\n",
    "\n",
    "corpus_folder_path = 'Corpus_Thesaurus_textfiles\\corpus_containing_pavement'\n",
    "\n",
    "corpus_sentences = []\n",
    "\n",
    "for file_name in os.listdir(corpus_folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder_path, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = nltk.sent_tokenize(text) \n",
    "            for sentence in sentences:\n",
    "                words = nltk.word_tokenize(sentence.lower())\n",
    "                words = [word for word in words if word not in stop_words] \n",
    "                corpus_sentences.append(words)\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_sentences, vector_size=200, window=5, min_count=3, workers=4, sg=1, epochs=100)\n",
    "\n",
    "\n",
    "for word in model.wv.index_to_key:\n",
    "    similar_words = model.wv.most_similar(word, topn=10)\n",
    "    synonyms[word] = [(similar_word[0], similar_word[1]) for similar_word in similar_words]\n",
    "\n",
    "output_file = 'd_r_thesaurus_containing_pavement.txt'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for word in synonyms:\n",
    "        similar_words = synonyms[word]\n",
    "        line = f'{word}: ' + ', '.join([f'{similar_word[0]}({similar_word[1]:.2f})' for similar_word in similar_words])\n",
    "        file.write(line + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
